{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Chunking with LangChain\n",
        "\n",
        "Installing the dependenices we'll be using to explore what Semantic Chunking is - and why it's useful!"
      ],
      "metadata": {
        "id": "5e7w3Lcl0J8X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFIF7UG2S5SA"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_experimental langchain_openai langchain_community langchain ragas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU faiss-cpu tiktoken"
      ],
      "metadata": {
        "id": "vqnY2Mybfxbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be working with \"Alice and Wonderland\" as our source material - let's load it into memory."
      ],
      "metadata": {
        "id": "KHOZSV522COR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.gutenberg.org/files/11/11-0.txt -O alice.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmaE2Z9Mbbix",
        "outputId": "3ac7731a-6012-4d08-f88d-cf87b2bc9f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-15 19:01:45--  https://www.gutenberg.org/files/11/11-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 154638 (151K) [text/plain]\n",
            "Saving to: ‘alice.txt’\n",
            "\n",
            "alice.txt           100%[===================>] 151.01K   579KB/s    in 0.3s    \n",
            "\n",
            "2024-04-15 19:01:45 (579 KB/s) - ‘alice.txt’ saved [154638/154638]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./alice.txt\") as f:\n",
        "  alice_in_wonderland = f.read()"
      ],
      "metadata": {
        "id": "7tGzjsEhcJgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RecursiveCharacterTextSplitter AKA \"Naive Chunking\"\n",
        "\n",
        "Let's look at our documents if we use a traditional non-semantic chunking strategy!\n",
        "\n",
        "> NOTE: The chunk size chosen here is purely for illustrative purposes."
      ],
      "metadata": {
        "id": "z3Y0H2fx2Sd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False\n",
        ")"
      ],
      "metadata": {
        "id": "AEFTjiyHbdim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_chunks = text_splitter.split_text(alice_in_wonderland)"
      ],
      "metadata": {
        "id": "vLrEftzob8_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in naive_chunks[10:15]:\n",
        "  print(chunk + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kov5hAYTcQZr",
        "outputId": "17414c7f-1efd-4d51-87f5-918fcf254ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dear! Oh dear! I shall be late!” (when she thought it over afterwards,\n",
            "it occurred to her that she ought to have wondered at this, but at the\n",
            "\n",
            "time it all seemed quite natural); but when the Rabbit actually _took a\n",
            "watch out of its waistcoat-pocket_, and looked at it, and then hurried\n",
            "\n",
            "on, Alice started to her feet, for it flashed across her mind that she\n",
            "had never before seen a rabbit with either a waistcoat-pocket, or a\n",
            "\n",
            "watch to take out of it, and burning with curiosity, she ran across the\n",
            "field after it, and fortunately was just in time to see it pop down a\n",
            "large rabbit-hole under the hedge.\n",
            "\n",
            "In another moment down went Alice after it, never once considering how\n",
            "in the world she was to get out again.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how our chunks wind up split across sentences, and we have similar context split across chunks as well.\n",
        "\n",
        "There are a number of strategies to counter this problem but we're going to focus on Semantic Chunking."
      ],
      "metadata": {
        "id": "bMNhSf5L25Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Chunking\n",
        "\n",
        "Let's start by providing our OpenAI API key.\n"
      ],
      "metadata": {
        "id": "sYvgnrkT3b-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wihQ77IxdDSt",
        "outputId": "797318f2-f443-4847-ad7f-e0bc1fe901d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's implement the `SemanticChunker`!\n",
        "\n",
        "We're going to be using the `percentile` threshold as an example - but there's three different strategies you could use (descriptions provided by the [LangChain docs](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker) on Semantic Chunking):\n",
        "\n",
        "- `percentile` (default) - In this method, all differences between sentences are calculated, and then any difference greater than the X percentile is split.\n",
        "\n",
        "- `standard_deviation` - In this method, any difference greater than X standard deviations is split.\n",
        "\n",
        "- `interquartile` - In this method, the interquartile distance is used to split chunks.\n",
        "\n",
        "The basic idea is as follows:\n",
        "\n",
        "1. Split our document into sentences (based on `.`, `?`, and `!`)\n",
        "2. Index each sentence based on position\n",
        "3. Add a `buffer_size` (`int`) of sentences on either side of our selected sentence\n",
        "4. Calculate distances between groups of sentences\n",
        "5. Merge groups based on similarity based on the above thresholds\n",
        "\n"
      ],
      "metadata": {
        "id": "Y6ChDQOz3pH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Initialize OpenAI Embeddings\n",
        "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", base_url=\"https://drchat.xyz\")\n",
        "\n",
        "# Initialize SemanticChunker with OpenAI Embeddings\n",
        "semantic_chunker = SemanticChunker(openai_embeddings, breakpoint_threshold_type=\"percentile\")\n"
      ],
      "metadata": {
        "id": "7aA0RVZaXaeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create our documents."
      ],
      "metadata": {
        "id": "39NvTTFT4LtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_chunks = semantic_chunker.create_documents([alice_in_wonderland])"
      ],
      "metadata": {
        "id": "Hwy45Au7dPX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the chunk associated with the above naive chunks.\n",
        "\n",
        "Notice how much more information is retained and included in each chunk.\n",
        "\n",
        "Also notice how much larger this chunk is!"
      ],
      "metadata": {
        "id": "3tX_XW1e5Fp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for semantic_chunk in semantic_chunks:\n",
        "  if \"waistcoat-pocket_\" in semantic_chunk.page_content:\n",
        "    print(semantic_chunk.page_content)\n",
        "    print(len(semantic_chunk.page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k66nWfvOdaI-",
        "outputId": "16640e7b-6a15-45cc-9900-f80ed1c4a2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿﻿*** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN\n",
            "WONDERLAND ***\n",
            "[Illustration]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Alice’s Adventures in Wonderland\n",
            "\n",
            "by Lewis Carroll\n",
            "\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\n",
            "\n",
            "Contents\n",
            "\n",
            " CHAPTER I. Down the Rabbit-Hole\n",
            " CHAPTER II. The Pool of Tears\n",
            " CHAPTER III. A Caucus-Race and a Long Tale\n",
            " CHAPTER IV. The Rabbit Sends in a Little Bill\n",
            " CHAPTER V. Advice from a Caterpillar\n",
            " CHAPTER VI. Pig and Pepper\n",
            " CHAPTER VII. A Mad Tea-Party\n",
            " CHAPTER VIII. The Queen’s Croquet-Ground\n",
            " CHAPTER IX. The Mock Turtle’s Story\n",
            " CHAPTER X. The Lobster Quadrille\n",
            " CHAPTER XI. Who Stole the Tarts? CHAPTER XII. Alice’s Evidence\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I. Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into\n",
            "the book her sister was reading, but it had no pictures or\n",
            "conversations in it, “and what is the use of a book,” thought Alice\n",
            "“without pictures or conversations?”\n",
            "\n",
            "So she was considering in her own mind (as well as she could, for the\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure of\n",
            "making a daisy-chain would be worth the trouble of getting up and\n",
            "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
            "close by her. There was nothing so _very_ remarkable in that; nor did Alice think it\n",
            "so _very_ much out of the way to hear the Rabbit say to itself, “Oh\n",
            "dear! Oh dear! I shall be late!” (when she thought it over afterwards,\n",
            "it occurred to her that she ought to have wondered at this, but at the\n",
            "time it all seemed quite natural); but when the Rabbit actually _took a\n",
            "watch out of its waistcoat-pocket_, and looked at it, and then hurried\n",
            "on, Alice started to her feet, for it flashed across her mind that she\n",
            "had never before seen a rabbit with either a waistcoat-pocket, or a\n",
            "watch to take out of it, and burning with curiosity, she ran across the\n",
            "field after it, and fortunately was just in time to see it pop down a\n",
            "large rabbit-hole under the hedge. In another moment down went Alice after it, never once considering how\n",
            "in the world she was to get out again. The rabbit-hole went straight on like a tunnel for some way, and then\n",
            "dipped suddenly down, so suddenly that Alice had not a moment to think\n",
            "about stopping herself before she found herself falling down a very\n",
            "deep well. Either the well was very deep, or she fell very slowly, for she had\n",
            "plenty of time as she went down to look about her and to wonder what\n",
            "was going to happen next. First, she tried to look down and make out\n",
            "what she was coming to, but it was too dark to see anything; then she\n",
            "looked at the sides of the well, and noticed that they were filled with\n",
            "cupboards and book-shelves; here and there she saw maps and pictures\n",
            "hung upon pegs. She took down a jar from one of the shelves as she\n",
            "passed; it was labelled “ORANGE MARMALADE”, but to her great\n",
            "disappointment it was empty: she did not like to drop the jar for fear\n",
            "of killing somebody underneath, so managed to put it into one of the\n",
            "cupboards as she fell past it. “Well!” thought Alice to herself, “after such a fall as this, I shall\n",
            "think nothing of tumbling down stairs! How brave they’ll all think me\n",
            "at home! Why, I wouldn’t say anything about it, even if I fell off the\n",
            "top of the house!” (Which was very likely true.)\n",
            "\n",
            "Down, down, down. Would the fall _never_ come to an end? “I wonder how\n",
            "many miles I’ve fallen by this time?” she said aloud. “I must be\n",
            "getting somewhere near the centre of the earth. Let me see: that would\n",
            "be four thousand miles down, I think—” (for, you see, Alice had learnt\n",
            "several things of this sort in her lessons in the schoolroom, and\n",
            "though this was not a _very_ good opportunity for showing off her\n",
            "knowledge, as there was no one to listen to her, still it was good\n",
            "practice to say it over) “—yes, that’s about the right distance—but\n",
            "then I wonder what Latitude or Longitude I’ve got to?” (Alice had no\n",
            "idea what Latitude was, or Longitude either, but thought they were nice\n",
            "grand words to say.)\n",
            "\n",
            "Presently she began again. “I wonder if I shall fall right _through_\n",
            "the earth! How funny it’ll seem to come out among the people that walk\n",
            "with their heads downward! The Antipathies, I think—” (she was rather\n",
            "glad there _was_ no one listening, this time, as it didn’t sound at all\n",
            "the right word) “—but I shall have to ask them what the name of the\n",
            "country is, you know. Please, Ma’am, is this New Zealand or Australia?”\n",
            "(and she tried to curtsey as she spoke—fancy _curtseying_ as you’re\n",
            "falling through the air! Do you think you could manage it?) “And what\n",
            "an ignorant little girl she’ll think me for asking!\n",
            "4623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a RAG Pipeline Utilizing Semantic Chunking\n",
        "\n",
        "Let's create a RAG LCEL chain that leverages our created Semantic Chunks.\n",
        "\n",
        "We'll start by creating our Retriever."
      ],
      "metadata": {
        "id": "5zpklrvY5RV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval\n",
        "\n",
        "We're going to use Meta's FAISS-backed vectorstore, and we'll use `text-embedding-3-large` (the same embedding model used to do the semantic chunking)\n"
      ],
      "metadata": {
        "id": "qij7VRIK5e9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", base_url=\"https://drchat.xyz\")\n",
        "semantic_chunk_vectorstore = FAISS.from_documents(semantic_chunks, embedding=openai_embeddings)"
      ],
      "metadata": {
        "id": "Iw7gxzirfsIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will \"limit\" our semantic retriever to `k = 1` to demonstrate the power of the semantic chunking strategy while maintaining similar token counts between the semantic and naive retrieved context."
      ],
      "metadata": {
        "id": "y9nEcnJ55yw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_chunk_retriever = semantic_chunk_vectorstore.as_retriever(search_kwargs={\"k\" : 1})"
      ],
      "metadata": {
        "id": "UmVEs8eBgP4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_chunk_retriever.invoke(\"Who has a pocket watch?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9tUKruGh-Zn",
        "outputId": "4dea4746-fd37-4425-f363-825ded24bc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='“He won’t stand beating. Now, if you only kept on good terms with him, he’d do almost anything\\nyou liked with the clock. For instance, suppose it were nine o’clock in\\nthe morning, just time to begin lessons: you’d only have to whisper a\\nhint to Time, and round goes the clock in a twinkling! Half-past one,\\ntime for dinner!”\\n\\n(“I only wish it was,” the March Hare said to itself in a whisper.)\\n\\n“That would be grand, certainly,” said Alice thoughtfully: “but then—I\\nshouldn’t be hungry for it, you know.”\\n\\n“Not at first, perhaps,” said the Hatter: “but you could keep it to\\nhalf-past one as long as you liked.”\\n\\n“Is that the way _you_ manage?” Alice asked. The Hatter shook his head mournfully.')]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented\n",
        "\n",
        "We'll create a classic RAG prompt to augment our question with the retrieved context."
      ],
      "metadata": {
        "id": "wfci5WGd6D5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_template = \"\"\"\\\n",
        "Use the following context to answer the user's query. If you cannot answer, please respond with 'I don't know'.\n",
        "\n",
        "User's Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(rag_template)"
      ],
      "metadata": {
        "id": "nOpS9jUSgc5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation\n",
        "\n",
        "We'll use the default `ChatOpenAI` model for our generator."
      ],
      "metadata": {
        "id": "iW82iOAH6JYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "base_model = ChatOpenAI(base_url=\"https://drchat.xyz\")"
      ],
      "metadata": {
        "id": "HUvkY6emg9Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LCEL Chain\n",
        "\n",
        "We'll create our classic LCEL chain here to test the RAG LCEL chain"
      ],
      "metadata": {
        "id": "7P2ff9zj6NDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "semantic_rag_chain = (\n",
        "    {\"context\" : semantic_chunk_retriever, \"question\" : RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | base_model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "dJaehLffhD-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "-w_p2KEY6SfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_rag_chain.invoke(\"How does Alice find herself falling down the rabbit hole into Wonderland?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "_PYkAZnphdyC",
        "outputId": "13d738d3-5f39-43e8-b848-11279ca19c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Alice finds herself falling down the rabbit hole into Wonderland when she sees a White Rabbit with pink eyes run by her. The Rabbit takes out a watch from its waistcoat-pocket, which surprises Alice. Curiosity gets the better of her, and she chases the Rabbit across the field until she sees it disappear into a large rabbit-hole under a hedge. Without thinking about how she will get out, Alice follows the Rabbit down the rabbit-hole and begins her journey into Wonderland.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_rag_chain.invoke(\"Who is Dinah, and what is their importance to Alice?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "328GZVL-jtcg",
        "outputId": "961c183d-2971-4ebd-84a0-ed4760818bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Dinah is Alice's cat and she is important to Alice because she is an excellent mouse catcher.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These answers seem great!\n",
        "\n",
        "Let's repeat this process for our naive chunking!"
      ],
      "metadata": {
        "id": "tIgOOzER6UdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_chunk_vectorstore = FAISS.from_texts(naive_chunks, embedding=openai_embeddings)"
      ],
      "metadata": {
        "id": "yRAHbTcsjQn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we're going to use `k = 15` here - this is to \"make it a fair comparison\" between the two strategies."
      ],
      "metadata": {
        "id": "VzULRPto8Awz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_chunk_retriever = naive_chunk_vectorstore.as_retriever(search_kwargs={\"k\" : 15})"
      ],
      "metadata": {
        "id": "YmEl9e3zjei5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_rag_chain = (\n",
        "    {\"context\" : naive_chunk_retriever, \"question\" : RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | base_model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "pGGxvjDFjlBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_rag_chain.invoke(\"How does Alice find herself falling down the rabbit hole into Wonderland?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OlIYioVojn45",
        "outputId": "ba37e813-163e-4e58-82b5-a3d2351065c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Alice finds herself falling down the rabbit hole into Wonderland when she sees a large rabbit-hole under the hedge and decides to chase after a rabbit that went down it.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_rag_chain.invoke(\"Who is Dinah, and what is their importance to Alice?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DPA2CxyJj0jX",
        "outputId": "52c808ba-d2d5-4e1d-b7e7-ffe8d9cd17f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Dinah is Alice's cat and holds importance to Alice as her beloved pet.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These answers are not bad - but they lack a certain depth that the previous answers did."
      ],
      "metadata": {
        "id": "8LrRmMFh77HF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "35rcP06y2XUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ragas Assessment Comparison\n",
        "\n",
        "Let's go ahead and leverage a great tool: [Ragas](https://docs.ragas.io/en/stable/getstarted/index.html)\n",
        "\n",
        "We're going to split our documents utilizing a different chunking strategy to avoid any \"cheating\" by the naive retriever."
      ],
      "metadata": {
        "id": "plxFmptX8JVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_data_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False\n",
        ")"
      ],
      "metadata": {
        "id": "dTsukcTBkB5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_data_chunks = synthetic_data_splitter.create_documents([alice_in_wonderland])"
      ],
      "metadata": {
        "id": "1fgqqFeski2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will create:\n",
        "\n",
        "- Questions - synthetically generated (`gpt-3.5-turbo`)\n",
        "- Contexts - created above\n",
        "- Ground Truths - synthetically generated (`gpt4-1106-preview`)\n",
        "- Answers - generated from our Semantic RAG Chain"
      ],
      "metadata": {
        "id": "-Q292bc58ZEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "ground_truths_semantic = []\n",
        "contexts = []\n",
        "answers = []\n",
        "\n",
        "question_prompt = \"\"\"\\\n",
        "You are a teacher preparing a test. Please create a question that can be answered by referencing the following context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "question_prompt = ChatPromptTemplate.from_template(question_prompt)\n",
        "\n",
        "ground_truth_prompt = \"\"\"\\\n",
        "Use the following context and question to answer this question using *only* the provided context.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "ground_truth_prompt = ChatPromptTemplate.from_template(ground_truth_prompt)\n",
        "\n",
        "question_chain = question_prompt | ChatOpenAI(model=\"gpt-3.5-turbo\", base_url=\"https://drchat.xyz\") | StrOutputParser()\n",
        "ground_truth_chain = ground_truth_prompt | ChatOpenAI(model=\"gpt4-1106-preview\", base_url=\"https://drchat.xyz\") | StrOutputParser()\n",
        "\n",
        "for chunk in synthetic_data_chunks[10:20]:\n",
        "  questions.append(question_chain.invoke({\"context\" : chunk.page_content}))\n",
        "  contexts.append([chunk.page_content])\n",
        "  ground_truths_semantic.append(ground_truth_chain.invoke({\"question\" : questions[-1], \"context\" : contexts[-1]}))\n",
        "  answers.append(semantic_rag_chain.invoke(questions[-1]))"
      ],
      "metadata": {
        "id": "uDU80s47kvM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll format those into a dataset!"
      ],
      "metadata": {
        "id": "f3xbLo9v83Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "qagc_list = []\n",
        "\n",
        "for question, answer, context, ground_truth in zip(questions, answers, contexts, ground_truths_semantic):\n",
        "  qagc_list.append({\n",
        "      \"question\" : question,\n",
        "      \"answer\" : answer,\n",
        "      \"contexts\" : context,\n",
        "      \"ground_truth\" : ground_truth\n",
        "  })\n",
        "\n",
        "eval_dataset = Dataset.from_list(qagc_list)"
      ],
      "metadata": {
        "id": "6lO0juSSzYWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL5aF3Ffz0QU",
        "outputId": "6e50a8fe-e6a8-4255-8873-4bb9d22c3255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
              "    num_rows: 10\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can implement Ragas metrics and evaluate our created dataset."
      ],
      "metadata": {
        "id": "rnXOZA7T85jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI"
      ],
      "metadata": {
        "id": "nQJxJGIGkLJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3 = OpenAI(model=\"gpt-3.5-turbo\", base_url=\"https://drchat.xyz\")"
      ],
      "metadata": {
        "id": "vMFcz-o5fSKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "gpt3_wrapper = LangchainLLMWrapper(gpt3)"
      ],
      "metadata": {
        "id": "FUWoYtd9f79i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "answer_relevancy.llm = gpt3\n",
        "faithfulness.llm = gpt3\n",
        "context_recall.llm = gpt3\n",
        "context_precision.llm = gpt3"
      ],
      "metadata": {
        "id": "Qcze7Yra0bMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_relevancy.embeddings = openai_embeddings\n",
        "faithfulness.embeddings = openai_embeddings\n",
        "context_recall.embeddings = openai_embeddings\n",
        "context_precision.embeddings= openai_embeddings"
      ],
      "metadata": {
        "id": "HxkI8qXOQ6f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer_relevancy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqvS_2dvMLuq",
        "outputId": "cc7d6d80-bbe2-4160-f2c5-efabaaef1486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnswerRelevancy(embeddings=<ragas.embeddings.base.LangchainEmbeddingsWrapper object at 0x7cc3df0b3670>, llm=OpenAI(client=<openai.resources.completions.Completions object at 0x7cc3ded437c0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7cc3eb21b250>, model_name='gpt-3.5-turbo', openai_api_key=SecretStr('**********'), openai_api_base='https://drchat.xyz', openai_proxy='', request_timeout=60), name='answer_relevancy', evaluation_mode=<EvaluationMode.qac: 1>, question_generation=Prompt(name='question_generation', instruction='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers', output_format_instruction='The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"object\", \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"]}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).', examples=[{'answer': 'Albert Einstein was born in Germany.', 'context': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time', 'output': {'question': 'Where was Albert Einstein born?', 'noncommittal': 0}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?', 'noncommittal': 0}}, {'answer': 'Everest', 'context': 'The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.', 'output': {'question': 'What is the tallest mountain on Earth?', 'noncommittal': 0}}, {'answer': \"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \", 'context': 'In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.', 'output': {'question': 'What was the groundbreaking feature of the smartphone invented in 2023?', 'noncommittal': 1}}], input_keys=['answer', 'context'], output_key='output', output_type='json', language='english'), strictness=3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "\n",
        "result = evaluate(\n",
        "    eval_dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_recall,\n",
        "    ], llm = gpt3,\n",
        "    embeddings=openai_embeddings\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "zM6Kgmbg1ou3",
        "outputId": "71e30716-5803-42f4-c911-d1913ab66ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'OpenAI' object has no attribute 'set_run_config'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-ff322852eaf3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m result = evaluate(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     metrics=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# initialize all the models in the metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     executor = Executor(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/evaluation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# initialize all the models in the metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     executor = Executor(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, run_config)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;34mf\"Metric '{self.name}' has no valid LLM provided (self.llm is None). Please initantiate a the metric with an LLM to run.\"\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             )\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_run_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'OpenAI' object has no attribute 'set_run_config'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "k1B-u2in2gmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}